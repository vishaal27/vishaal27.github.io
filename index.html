<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <title>Vishaal Udandarao</title>
  
  <meta name="author" content="Vishaal Udandarao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/v_.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Vishaal Udandarao</name>
              </p>
              <p>I am a final year <a href="https://ellis.eu/phd-postdoc">ELLIS</a> PhD student, jointly working with <a href="https://scholar.google.de/citations?user=0z0fNxUAAAAJ&hl=en">Matthias Bethge</a> at The University of Tuebingen and <a href="http://samuelalbanie.com/">Samuel Albanie</a> at The University of Cambridge/Google DeepMind. My research is funded by a <a href="https://research.google/programs-and-events/phd-fellowship/recipients/?filtertab=2024">Google PhD Fellowship in Machine Intelligence</a>.  
                <!-- I am also a part of the <a href="https://imprs.is.mpg.de/">International Max Planck Research School for Intelligent Systems</a>. I am mainly interested in understanding the generalisation properties of foundation models, both vision-language models (VLMs) and large multi-modal models (LMMs), through the lens of their pre-training and test data distributions. My key research interest-axes are: <em>Data-centric Machine Learning</em>, <em>Robustness/Generalisation to Distribution Shifts</em>, and <em>Foundation Models</em>.  -->
              </p>
              <p>
                I am very interested in data curation methods to improve multimodal pretraining. During my PhD, I have primarily worked on <i>characterizing and constructing better pretraining datasets for multimodal models</i>. 
              </p>
              <p> I previously interned in Google (DeepMind) and Apple, working on data-curation for multimodal models. 
                <!-- with <a href="https://xianyongqin.github.io/">Yongqin Xian</a>, <a href="https://alessiotonioni.github.io/">Alessio Tonioni</a>, <a href="https://federicotombari.github.io/">Federico Tombari</a>, and <a href="https://www.olivierhenaff.com/">Olivier Henaff</a>. I also closely collaborated with <a href="https://ferjad.github.io/">Ferjad Naeem</a>, <a href="https://scholar.google.com/citations?user=X9mO4ckAAAAJ&hl=en">Nikhil Parthasarathy</a> and <a href="https://talfanevans.co.uk/">Talfan Evans</a>. -->
              </p>
              <!-- <p> -->
                <!-- Previously, I was an <a href="https://www.mlmi.eng.cam.ac.uk/">MPhil Machine Learning and Machine Intelligence</a> student at The University of Cambridge. My thesis was on <a href="https://www.mlmi.eng.cam.ac.uk/files/2021-2022_dissertations/understanding_and_fixing_the_modality_gap_in_vision-language_models_reduced.pdf">Understanding and Fixing the Modality Gap in VLMs</a>. I graduated from <a href="http://iiitd.ac.in/">IIIT Delhi</a> with a Bachelors in Computer Science in July, 2020. -->
              <!-- </p> -->
              <!-- <p> -->
                <!-- I am also fortunate to have previously worked with several great mentors: <a href="https://ankushgupta.org/">Ankush Gupta (Google Deepmind)</a>, <a href="https://sungjinahn.com/">Sungjin Ahn (KAIST)</a>, <a href="https://www.tanmoychak.com/">Tanmoy Chakraborty (IIT Delhi)</a>, <a href="https://www.iiitd.ac.in/rajivratn">Rajiv Ratn Shah (IIIT Delhi)</a>, <a href="https://faculty.iiitd.ac.in/~anands/">Saket Anand (IIIT Delhi)</a>, <a href="https://sites.google.com/view/kumar7">Rajesh Kumar (Bucknell University)</a>, <a href="https://www.iiitd.edu.in/~anubha/">Anubha Gupta (IIIT Delhi)</a> and <a href="https://www.iiitd.ac.in/jainendra">Jainendra Shukla (IIIT Delhi)</a>. -->
              <!-- </p> -->
              <p><b>I'm currently on the industry job market! Please reach out if you have any open positions in pretraining.</b></p>
              
              <p style="text-align:center">
                <a href="mailto:vishaal16119@iiitd.ac.in">Email</a> &nbsp/&nbsp
                <a href="data/Vishaal_CV_jan_26.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=jUOcawkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/vishaal_urao">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/vishaal27">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IMG_4009.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_4009.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <p>For an updated list, please see my <a href="https://scholar.google.com/citations?user=jUOcawkAAAAJ&hl=en">google scholar</a></p>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 

            <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2408.14471">
                <papertitle>Data-Centric Lessons To Improve Speech-Language Pretraining</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao</strong>, <a href="https://scholar.google.com/citations?user=9085klwAAAAJ&hl=en">Zhiyun Lu</a>, <a href="https://scholar.google.com/citations?user=cIl2jpMAAAAJ&hl=zh-CN">Xuankai Chang</a>, <a href="https://scholar.google.com/citations?user=OkngbkkAAAAJ&hl=en">Yongqiang Wang</a>, <a href="https://scholar.google.com/citations?user=DNcd76sAAAAJ&hl=en">Violet Z. Yao</a>, <a href="https://dblp.org/pid/338/9642.html">Albin Madapally Jose</a>, <a href="https://fartashf.github.io/">Fartash Faghri</a>, <a href="https://jpgard.github.io/">Josh Gardner</a>, <a href="https://scholar.google.com/citations?hl=en&user=ixhmT3AAAAAJ&view_op=list_works&sortby=pubdate">Chung-Cheng Chiu</a>
              <br>
              <em>ICLR</em>, 2026
              <br>
              <a href="https://arxiv.org/pdf/2510.20860">pdf</a> /
              <!-- <a href="https://github.com/ExplainableML/fomo_in_flux">code</a> -->
              <p>We studied three questions fundamental to speech-language pretraining data curation and sampling; our controlled experiments yielded a performant 3.8B SpeechLM that outperforms 3x-larger SpeechLMs.</p>
            </td>
          </tr>
          
            <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2408.14471">
                <papertitle>A Practitioner’s Guide to Continual Multimodal Pretraining</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://karroth.com/">Karsten Roth*</a>, <a href="https://sebastiandziadzio.com/about/">Sebastian Dziadzio</a>, <a href="https://drimpossible.github.io/">Ameya Prabhu</a>, <a href="https://mehdidc.github.io/">Mehdi Cherti</a>, <a href="https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en">Oriol Vinyals</a>, <a href="https://www.olivierhenaff.com/">Olivier Henaff</a>, <a href="https://samuelalbanie.com/">Samuel Albanie</a>, <a href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ">Matthias Bethge*</a>, <a href="https://www.eml-unitue.de/people/zeynep-akata">Zeynep Akata*</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2408.14471">pdf</a> /
              <a href="https://github.com/ExplainableML/fomo_in_flux">code</a>
              <p>We provide practical insights into how to continually pretraining contrastive multimodal models under compute and data constraints.</p>
            </td>
          </tr>
          
          
            <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2404.04125">
                <papertitle>No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://drimpossible.github.io/">Ameya Prabhu*</a>, <a href="https://adhirajghosh.github.io/">Adhiraj Ghosh</a>, <a href="https://www.yash-sharma.com/">Yash Sharma</a>, <a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a>, <a href="https://www.adelbibi.com/">Adel Bibi</a>, <a href="https://samuelalbanie.com/">Samuel Albanie</a>, <a href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ">Matthias Bethge</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2404.04125">pdf</a> /
              <a href="https://github.com/bethgelab/frequency_determines_performance">code</a>
              <p>Our work showcases that the impressive empirical performance of multimodal models like CLIP and Stable Diffusion can be largely attributed to the presence of test concepts within their vast pretraining datasets, thus their reported empirical performance does not constitute "zero-shot" generalization. Quite the contrary, these models require exponentially more data on a concept to linearly improve their performance on tasks pertaining to that concept, highlighting extreme sample inefficiency.</p>
            </td>
          </tr>
          
            <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2402.19472">
                <papertitle>Efficient Model Evaluation in an Era of Rapid Progress</papertitle>
              </a>
              <br>
              <a href="https://drimpossible.github.io/">Ameya Prabhu*</a>, <strong>Vishaal Udandarao*</strong>, <a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a>, <a href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ">Matthias Bethge</a>, <a href="https://www.adelbibi.com/">Adel Bibi</a>, <a href="https://samuelalbanie.com/">Samuel Albanie</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2402.19472">pdf</a> /
              <a href="https://github.com/bethgelab/sort-and-search">code</a>
              <p>Our work introduces the concept of lifelong benchmarks, enabling effective comparisons of models and reducing overfitting to the biases of a particular dataset. We constructed large-scale lifelong classification benchmarks totalling over 1.5M samples. To facilitate more efficient evaluation, we introduce the Sort&Search method that reduces inference compute costs by 1000x.</p>
            </td>
          </tr>

            <!-- <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2407.12861">
                <papertitle>CiteME: Can Language Models Accurately Cite Scientific Claims?</papertitle>
              </a>
              <br>
              <a href="https://oripress.com/">Ori Press*</a>, <a href="https://scholar.google.de/citations?user=tc8HH6MAAAAJ&hl=en">Andreas Hochlehnert*</a>, <a href="https://drimpossible.github.io/">Ameya Prabhu</a>, <strong>Vishaal Udandarao</strong>, <a href="https://ofir.io/">Ofir Press</a>, <a href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ">Matthias Bethge</a>
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2407.12861">pdf</a> /
              <a href="https://www.citeme.ai/">web</a>
              <p>CiteME is a benchmark designed to test the abilities of language models in finding papers that are cited in scientific texts.</p>
            </td>
          </tr> -->
          
          
            <!-- <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2310.08577">
                <papertitle>Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://scholar.google.de/citations?user=-T_5tc0AAAAJ&hl=en">Max F. Burg*</a>, <a href="samuelalbanie.com">Samuel Albanie</a>, <a href="https://scholar.google.com/citations?user=0z0fNxUAAAAJ">Matthias Bethge</a>
              <br>
              <em>ICLR</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2310.08577.pdf">pdf</a> /
              <a href="https://github.com/bethgelab/DataTypeIdentification">code</a>
              <p>We introduce "Visual Data-Type Identification": the task of classifying between visual image distortions and styles. On this simple task, we find surprising behaviour of VLMs and LMMs: model scaling does not significantly improve performance. We trace this behaviour back to the LAION-2B dataset and show a simple fine-tuning method to improve performance.</p>
            </td>
          </tr> -->
          
            <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle"  >
              <img src="images/sus-x_sq.jpg" alt="sus-x" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2211.16198">
                <papertitle>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao</strong>, <a href="https://ankushgupta.org/">Ankush Gupta</a>, <a href="samuelalbanie.com">Samuel Albanie</a>
              <br>
              <em>ICCV</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2211.16198.pdf">pdf</a> /
              <a href="https://github.com/vishaal27/sus-X/">code</a>
              <p>We enhance CLIP's downstream classification performance by (1) curating a support set either by generating synthetic (Stable Diffusion) or retrieving natural (LAION-5B) samples, and (2) observing and fixing a mis-calibration issue with intra-modal distances in CLIP’s embedding space.</p>
            </td>
          </tr>
          
            <!-- <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/levasa_sq.jpg" alt="levasa" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2007.10058">
                <papertitle>It's LeVAsa not LevioSA! Latent Encodings for Valence-Arousal Structure Alignment</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://surabhisnath.github.io/index.html">Surabhi Nath*</a>, <a href="https://jainendra.in/">Jainendra Shukla</a>
              <br>
              <em>CODS-COMAD</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2007.10058.pdf">pdf</a> /
              <a href="https://github.com/vishaal27/LeVAsa">code</a>
              <p>A VAE model that learns implicit structure by aligning the latent space with the Valence-Arousal circumplex space. Further, a novel algorithm for mapping categorical and dimensional model labels using annotation transfer across affective facial image datasets is depicted.</p>
            </td>
          </tr>
          
          <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cobra_sq.jpg" alt="cobra" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2005.03687">
                <papertitle>COBRA: Contrastive Bi-Modal Representation Algorithm</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="http://ovshake.me/">Abhishek Maiti*</a>, <a href="https://suryatejreddy.github.io/">Suryatej Reddy Vyalla*</a>, <a href="https://scholar.google.com/citations?user=DlLbu9UAAAAJ&hl=en">Deepak Srivatsav*</a>, <a href="https://scholar.google.com.sg/citations?user=TRfTdBAAAAAJ&hl=en">Yifang Yin</a>, <a href="https://www.iiitd.edu.in/~rajivratn/">Rajiv Ratn Shah</a>
              <br>
              <em>TUSION workshop, IJCAI</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2005.03687.pdf">pdf</a> /
              <a href="https://github.com/ovshake/cobra">code</a>
              <p>A novel bi-modal framework that aims to train two modalities (image and text) in a joint fashion inspired by the Contrastive Predictive Coding (CPC) and Noise Contrastive Estimation (NCE) paradigms which preserve both inter and intra-class relationships in a modality-invariant fashion.</p>
            </td>
          </tr>   

          
           <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/inphynet_sq.jpg" alt="inphynet" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512030616X">
                <papertitle>InPHYNet: Leveraging Attention-based Multitask Recurrent Networks for Multi-label Physics Text Classification</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, Abhishek Agarwal*, <a href="https://www.iiitd.edu.in/~anubha/">Anubha Gupta</a>, <a href="http://faculty.iiitd.ac.in/~tanmoy/">Tanmoy Chakraborty</a>
              <br>
              <em>Knowledge-Based Systems</em>, 2020
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S095070512030616X">pdf</a> /
              <a href="https://github.com/abhishekag03/InPHYNet">code</a>
              <p>A multi-task learning model which incorporates auxiliary semantics by utilising a weight alignment layer and information exchange layer.</p>
            </td>
          </tr>
          
           <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/discont_sq.jpg" alt="discont" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2006.05895">
                <papertitle>DisCont: Self-Supervised Visual Attribute Disentanglement using Context Vectors</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://sarthak268.github.io/">Sarthak Bhagat*</a>, <a href="https://shagunuppal.github.io/">Shagun Uppal*</a>, <a href="https://faculty.iiitd.ac.in/~anands/">Saket Anand</a>
              <br>
              <em>PTSGM Workshop, ECCV</em>, 2020, <em>MLI4SD Workshop, ICML</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2006.05895.pdf">pdf</a> /
              <a href="https://sarthak268.github.io/discont/">project page</a> /
              <a href="https://docs.google.com/presentation/d/1Qa3DiLQiJ_JpPk2Nx1v4QmcjIx5AsESNbZLUJ8rmD8A/edit">slides</a> /
              <a href="https://www.youtube.com/watch?v=lIYCMHesVBw&feature=youtu.be">video</a> /
              <a href="https://github.com/sarthak268/DisCont">code</a>
              <p>A self-supervised framework to disentangle multiple attributes by exploiting structural inductive biases within images and leveraging contrastive learning paradigms.</p>
            </td>
          </tr>

           <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/privacy-leaks_sq.jpg" alt="privacy-leak" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2006.09501">
                <papertitle>On the Inference of Soft Biometrics from Typing Patterns Collected in a Multi-device Environment</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://sites.google.com/view/mohit-agrawal/home/">Mohit Agrawal*</a>, <a href="https://sites.google.com/view/kumar7">Rajesh Kumar</a>, <a href="https://www.iiitd.edu.in/~rajivratn/">Rajiv Ratn Shah</a>
              <br>
              <em>BigMM</em>, 2020
              <br>
              <a href="https://conferences.computer.org/bigmm/pdfs/BigMM2020-13PaMgtIXlZNQOagcn4uc7/932500a076/932500a076.pdf">pdf</a> /
              <a href="https://github.com/midas-research/privacy-leaks">code</a>
              <p>An empirical study on the inference of gender, major/minor (computer science, non-computer science), typing style, age, and height from the typing patterns collected from 117 individuals in a multi-device environment.</p>
            </td>
          </tr>          
          
           <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/memeify_sq.jpg" alt="memeify" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1910.12279">
                <papertitle>Memeify: A Large-Scale Meme Generation System</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, <a href="https://suryatejreddy.github.io/">Suryatej Reddy Vyalla*</a>, <a href="http://faculty.iiitd.ac.in/~tanmoy/">Tanmoy Chakraborty</a>
              <br>
              <em>CODS-COMAD</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/1910.12279.pdf">pdf</a> /
              <a href="https://docs.google.com/presentation/d/1-u7WvZBYdiz2D1j08YGXPrvDiVSgDvJqb20KuY6Fs98/edit?usp=sharing">slides</a> /
              <a href="https://www.youtube.com/watch?v=P_Tfs0X-czs">video</a> /
              <a href="https://github.com/suryatejreddy/Memeify">code</a>
              <p>A meme generation system that uses a trained state-of-the-art transformer-based (GPT-2) model for caption generation by employing an encoder-decoder architecture.</p>
            </td>
          </tr>          
          
           <tr>
<!--             <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eduqa_sq.jpg" alt="eduqa" width="250" height="250">
            </td> -->
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1911.05013">
                <papertitle>EDUQA: Educational Domain Question Answering System using Conceptual Network Mapping</papertitle>
              </a>
              <br>
              <strong>Vishaal Udandarao*</strong>, Abhishek Agarwal*, Nikhil Sachdeva*, Raj Kamal Yadav*, Vrinda Mittal*, <a href="https://www.iiitd.edu.in/~anubha/">Anubha Gupta</a>, Abhinav Mathur
              <br>
              <em>ICASSP</em>, 2019
              <br>
              <a href="https://arxiv.org/pdf/1911.05013.pdf">pdf</a> /
              <a href="https://drive.google.com/file/d/1Gie8Os1v8OUUo6QV5c0oHYzIePMVIr4y/view?usp=sharing">poster</a>
              <p> An on-the-fly conceptual network model that incorporates educational semantics and preserves correlations between conceptual entities by applying intelligent indexing algorithms on an inherent concept network so as to improve answer generation.</p>
            </td>
          </tr> -->
          
           
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr> <td>
  <heading>Teaching</heading>
  </td> </tr> </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <!-- project begin -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:top">
                <papertitle>Deep Learning (CSE641)</papertitle>
              <br/>
              Worked as a Teaching Assistant for the Deep Learning course offered by <a href="https://faculty.iiitd.ac.in/~anands/">Dr. Saket Anand</a> in Spring 2020.
            </td>
          </tr>
    <!-- project end -->

      
    <!-- project begin -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:top">
                <papertitle>Machine Learning (CSE543)</papertitle>
              <br/>
              Worked as a Teaching Assistant for the Machine Learning course offered by <a href="https://jainendra.in/">Dr. Jainendra Shukla</a> in Fall 2019.
            </td>
          </tr>
    <!-- project end -->
      
          <!-- project begin -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:top">
                <papertitle>Introduction to Engineering Design (DES130)</papertitle>
              <br/>
              Worked as a Teaching Assistant for the Introduction to Engineering Design course offered by <a href="https://amanparnami.com/">Dr. Aman Parnami</a> in Spring 2019.
            </td>
          </tr>
    <!-- project end -->
      
      
          <!-- project begin -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:top">
                <papertitle>Linear Algebra (MTH100)</papertitle>
              <br/>
              Worked as a Teaching Assistant for the Linear Algebra course offered by Dr. Samaresh Chatterjee in Fall 2018.
            </td>
          </tr>
    <!-- project end -->

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr> <td>
  <heading>Misc</heading>
  </td> </tr> </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <p>
                Apart from my academic interests, I am a huge football fan and actively support <s>FC Barcelona</s> <s>Paris Saint Germain</s> Inter Miami CF. You've probably guessed already, Lionel Messi is my favourite player to ever touch a football. I also love watching Formula 1 and look up to Lewis Hamilton. I used to <a href="data/vishaal_poems.pdf">write</a> stuff, but that was a long long time ago. I also dabble around with the guitar and the keyboard at times. Checkout my <a href="https://soundcloud.com/vishaal-udandarao">soundcloud profile</a>!
              </p>
      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
                Website template taken from <a href="https://github.com/jonbarron/website/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>      
</body>

</html>
